---
published: true
layout: post
date: '2022-03-11 18:10:00 +0700'
tags:
  - Object Detection
description: FCOS 논문 리뷰
categories: jekyll update
comments: true
use_math: true
title: 'FCOS: Fully Convolutional One-Stage Object Detection'
---




이번에 리뷰할 논문은 바로 **[FCOS: Fully Convolutional One-Stage Object Detection]** 입니다. 이 논문은 2019년에 나왔으며, **Anchor box 기반 검출기 만큼의 정확도를 달성하는 Anchor Free 기반 검출기**의 baseline이 되는 논문이기 때문에 리뷰하게 되었습니다! (사실 CornerNet이 먼저이긴 하지만, CornerNet은 더 복잡한 post-processing 절차가 필요하다고 하네요!)

기존 객체 검출(Object Detection) 분야에서 대장을 이루었던 YOLO v3를 비롯하여 RetinaNet, SSD, Faster R-CNN 등의 객체 검출기들은 Anchor box를 사용하는데 비해, FCOS는 One-stage Detector이기 때문에 Anchor box를 사용하지 않습니다. 

FCOS는 미리 정의가 필요한 Anchor box를 사용하지 않기 때문에 최종 검출 성능에 영향을 미칠 수 있는 Anchor box와 관련된 하이퍼 파라미터들도 사용할 필요가 없습니다. 또한 후처리(post-processing)로는 유일하게 NMS(Non-Maximum Suppression)만을 사용하여 ResNeXt-64x4d-101을 사용하는 FCOS는 single-model, sigle-scale testing에서 AP 44.7%을 달성했다고 하네요! 그럼 Anchor box를 사용하지 않고도 어떻게 객체 검출을 하였는지 살펴봅시다 😋


# Introduction

객체 검출은 컴퓨터 비전에서 기본적이지만 어려운 작업이며, 객체 검출 알고리즘은 이미지에서 관심있는 각 인스턴스(Instance)에 대해 어떤 객체인지 분류하며, 그 객체가 이미지에서 어디에 위치하고 있는지 bounding box로 예측을 하게 됩니다. 

Faster R-CNN, SSD, YOLO v2, v3 과 같은 현재 나와있는 객체 검출기들은 미리 정의된 Anchor box 세트에 의존하는 경향이 있으며, 여태껏 Anchor box의 사용이 객체 검출의 핵심이라고 여겨져왔습니다. 하지만 이러한 검출기는 몇 가지 단점이 존재합니다. 

📌 **Anchor box의 크기(size), aaspect ratio, Anchor 갯수에 민감하다는 점**

예를 들면 RetinaNet에서 Anchor와 관련된 하이퍼 파라미터를 변경하면 COCO 벤치마크에서 AP 4% 정도의 영향을 미친다고 합니다. 그래서 이러한 Anchor 기반 객체 검출기들은 신중하게 조정될 필요가 있었습니다. 

📌 **Anchor의 scale과 aspect ratio가 고정되어 있다는 점**

그래서 작은 객체의 경우 모양 변화(shape variation)가 큰 객체의 후보(candidate)를 처리하는데 어려움을 겪게 됩니다. 이러한 다양한 객체 크기나 종횡비를 가진 작업에 대해 Anchor box들을 다시 설정 해야하기 때문에 "일반화"하기 힘들다는 단점이 있습니다. 

📌 **High recall rate를 위해 입력 영상에 Anchor box를 조밀하게 배치해야 한다는 점**

예를 들어 단축(짧은 변)이 800인 이미지의 경우 FPN(Feature Pyramid Networks)은 약 180K 이상의 Anchor 가 필요하게 됩니다. 이러한 Anchor box들은 대부분 학습 중에 negative sample로 레이블이 지정됩니다. 이러한 과도한 negative sample들은 학습에서 positive sample 과 negative sample 간의 불균형을 야기하게 됩니다. 

📌 **Anchor box는 ground-truth bounding box를 사용하여 IoU(Intersection-over-union) 계산을 한다는 점**

이는 아주 복잡한 계산을 많이 하게 됩니다. 

또한 최근 FCN(Fully Convolutional Networks)는 semantic segmentation, depth estimation, keypoint detection, counting 분야에서 엄청난 성능을 보여주었습니다. high-level vision tasks 중 하나인 객체 검출은 Anchor box의 사용으로 인하여 위와 같은 FCN 계열에 껴들 수 없는 존재였습니다. **그래서 본 논문에서는 아주 심플한 FCN 기반 검출기가 Anchor 기반 검출기보다 훨쓴 더 나은 성능을 달성한다는 것을 보여주게 됩니다!**

이러한 FCN 기반 프레임워크는 feature map 단계의 spatial location에서 4D vector와 class category를 직접 예측하게 됩니다. 다음 그림 1의 왼쪽에서 볼 수 있듯 4D vector는 bounding box의 네 개의 면에서 객체의 위치 까지 상대적인 offset을 나타냅니다. 이러한 프레임워크는 각 위치가 4D continuous vector를 regress 해야한다는 점을 제외하고서 semantic segmentation을 위한 FCN과 유사한 모습을 보여주게 됩니다. 

![Figure 1](/assets/img/paper_images/paper_post_2/fcos_figure_1.PNG)

유사한 연구 DenseBox 방법은 다른 크기의 bounding box를 처리하기 위해 학습 영상을 고정된 크기로 자르고 크기를 조정하게 됩니다. 따라서 DenseBox는 image pyramid 에 대한 검출을 수행해야하며, 이는 모든 convolution을 한 번 계산한다는 철학에 어긋나게 됩니다. 참고로 이러한 방법은 scene text detection, face detection과 같이 highly overlapped bounding boxes 가 존재하는 특수한 도메인에서 사용됩니다. 그림 1의 오른쪽에서 볼 수 있듯 중첩이 상당히 일어난 bounding box는 겹치는 영역의 픽셀에 대해 regress 할 bounnding box를 결정하기 어렵다는 문제가 있습니다. 

이 논문에서는 이러한 문제를 자세히 다루고, FPN을 사용하여 이러한 어려운 문제를 해결할 수 있음을 보여주며, 결과적으로 Anchor 기반 검출기와 유사한 정확도를 얻을 수 있다는 것을 보여줍니다. 또한 **이러한 방법이 대상 객체 중심에서 멀리 떨어진 위치에서 low-quality predicted bounding box를 생성할 수 있다는 것을 발견하였고, 이러한 문제를 해결하기 위해 해당 bounding box의 중심에 대한 픽셀의 편차(deviation)를 예측하는 "centerness" 개념을 소개합니다.** 이 score는 low-quality를 가지는 bounding box의 weight를 낮추고, NMS에서 검출 결과를 병합하는데 사용됩니다. 이렇게 간단하면서도 효과적인 centerness branch를 통해 FCN 기반 검출기는 Anchor 기반 검출기보다 성능이 뛰어나다는 것을 보여주게 됩니다. 


FCOS의 장점을 정리하자면 다음과 같습니다. 

📌 객체 검출 문제는 semantic segmentation 등과 같이 FCN 프레임워크를 사용할 수 있게 되므로, 다양한 작업들과 통합하여 재사용 될 수 있습니다. 

📌 **Proposal Free 및 Anchor Free 라서 파라미터의 수가 크게 줄어듭니다.** 이러한 파라미터들은 heuristic tuning 이 필요하며, 좋은 성능을 달성하기 위해서 많은 trick이 필요합니다. 또한 Anchor Free 기반 검출기는 학습이 비교적 단순해집니다!

📌 Anchor box를 제거함으로써 IoU 계산과 같은 Anchor box와 관련된 복잡한 계산들을 더 이상 하지 않아도 되고, 학습 중 Anchor box와 GT-box 간의 매칭을 할 필요가 없으므로 **더 빠른 학습과 테스트를 수행** 할 수 있습니다. 

📌 **FCOS는 SOTA를 달성합니다.** 또한 이 FCOS는 two-stage 검출기에서 RPN(Region Proposal Networks)로 사용 될 수 있으며, Anchor 기반 RPN 방법 보다 훨씬 더 나은 성능을 보장합니다. 

📌 제안된 검출기는 instance segmentation 및 keypoint detection을 포함하여 **최소한의 수정으로 다른 vision 관련 task들을 해결 할 수 있도록 확장이 가능**합니다. 그래서 FCOS는 many instance-wise prediction 문제에 대한 새로운 baseline이 될 것 이라고 하네요!



# Related Work

### Anchor-based Detectors
Anchor 기반 검출기는 Fast R-CNN과 같은 전통적인 sliding-window 및 proposal 기반 검출기의 아이디어에서 시작하며, Anchor box는 bounding box 위치 예측을 개선시키기 위해 extra offsets regression과 함께 positive 또는 negative patch로 분류되는 pre-defined sliding windows 또는 proposal으로 정의 됩니다. 따라서 이러한 검출기의 Anchor box는 training sample로 볼 수 있으며, 각 sliding windows/proposal의 image feature를 반복적으로 계산하는 Fast RCNN과 같은 검출기와는 달리 Anchor box는 CNN의 feature map을 사용하고 반복적인 feature 계산을 하지 않고 검출 프로세스 속도를 크게 높이게 됩니다. Anchor box를 설계하는 것은 Faster R-CNN의 RPN, SDD, Yolo v2에 의해 대중화되었으며 검출 모델의 baseline이 되어왔습니다. 

그러나 위에서 설명한 것 처럼 Anchor box는 지나치게 많은 하이퍼 파라미터들을 생성하게 되며 일반적으로 우수한 성능을 달성하기 위해서는 미세하게 이를 조정해야한다는 단점이 있습니다. Anchor shape을 설명하는 하이퍼 파라미터 외에도 Anchor 기반 검출기는 Anchor box에 positive, ignored 또는 negative sample로 레이블을 지정하는 하이퍼 파라미터도 필요합니다. 이전에는 Anchor box의 레이블을 결정하기 위해 Anchor box와 GT-box 사이의 IoU를 사용하게 됩니다. 이러한 파라미터는 최종 정확도에 큰 영향을 미치게 되며 heuristic tuning이 필요하게 됩니다.  

### Anchor-free Detectors
가장 널리 알려진 Anchor-free 검출기는 YOLO v1 일 수 있습니다. Anchor box를 사용하는 대신 YOLO v1은 객체 중심 근처 지점에서 bounding box를 예측하게 됩니다. 더 높은 품질의 검출을 생성할 수 있는 것으로 간주되기 때문에 중앙에서 가까운 지점만을 사용하게 됩니다. 그러나 이는 중심 근처의 지점만 bounding box를 예측하는데 사용되기 때문에 YOLO v1은 YOLO v2 에서 언급한 것 처럼 low recall을 보입니다. 결과적으로 YOLO v2는 Anchor box를 사용합니다. YOLO v1과 비교하여 FCOS는 GT box의 모든 지점을 이용하여 bounding box를 예측하고, low-quality의 bounding box는 제안된 "centerness"로 처리 됩니다. 결과적으로 FCOS는 Anchor 기반 검출기와 비슷한 recall을 제공합니다. 
CornerNet은 bounding box의 pair of corners를 검출하고 그룹화 하여 객체를 검출하는 SOTA Anchor-free 검출기입니다. 이는 동일한 인스턴스에 속하는 pair of corners를 그룹화 하기 위해 훨씬 더 복잡한 post-processing을 거치게 됩니다. 
또한 DenseBox 라는 것도 있습니다. 이는 overlapping bounding box를 처리하기 어렵고 recall이 상대적으로 낮기 때문에 일반 객체 검출에 적합하지 않은 것으로 나타났습니다. 



# Our Approach

FCOS는 per-pixel prediction 방식으로 Object Detection을 재구성하며, multi-level prediction을 사용하여 recall을 개선하고 overlapped bounding box로 인한 모호성을 해결하는 방법을 제안합니다. 마지막으로 low-quality detected bounding boxes를 억제하고 전체 성능을 크게 향상시키는 "centerness" 개념을 소개합니다. 

## Fully Convolutional One-Stage Object Detector
먼저 $F_{i}\in \mathbb{R^{H\times W \times C}}$를 backbone CNN의 layer $i$의 feature map 이라고 정의하고, $s$를 layer 까지의 stride 라고 정의합니다. 입력 영상에 대한 ground-truth bounding boxes는 $\left\{B_{i} \right\}$라고 정의하고 여기서 $B_{i} = (x_{0}^{(i)}, y_{0}^{(i)}, x_{1}^{(i)}, y_{1}^{(i)}, c^{i}) \in \mathbb{R}^{4}\times \left\{1, 2, ... C \right\}$ 입니다. 또한 $(x_{0}^{(i)}, y_{0}^{(i)})$과 $(x_{1}^{(i)}, y_{1}^{(i)})$은 bounding box의 left-top과 right-bottom corner의 좌표를 뜻합니다. $c^{i}$는 bounding box에 속하는 객체의 class를 뜻합니다. $C$는 class의 number를 뜻하며, MS-COCO 데이터세트에서는 80을 뜻합니다. 

각 feature map $F_{i}$위치인 location $(x, y)$는 $(x, y)$의 receptive field의 center인 $(\left \lfloor \frac{s}{2}\right \rfloor + xs, \left \lfloor \frac{s}{2}\right \rfloor + ys)$로 나타낼 수 있습니다.

입력 영상의 위치를 Anchor box의 중심으로 간주하여 대상 bounding box를 regress 하는 Anchor 기반 검출기와는 달리, FCOS는 해당 위치에서 대상 bounding box를 직접 regress 합니다. 즉, semantic segmentation을 위한 FCN과 동일한 Anchor 기반 검출기에서 Anchor box 대신 training samples 로 위치를 직접 다루게 됩니다. 

위치 location $(x, y)$가 GT box 에 속하고 클래스 레이블 $c^{*}$가 GT box의 클래스 레이블인 경우 ppositiva sample로 간주됩니다. 그렇지 않으면 negative sample이 됩니다. 여기서 $c^{*}=0$은 background class를 뜻합니다. 클래스 분류를 위한 레이블 외에도 위치 정보에 대한 regression target인 4D real vector $t^{*}=(l^{*}, t^{*}, r^{*}, b^{*})$ 가 있습니다. 여기서 $l^{*}, t^{*}, r^{*}, b^{*}$ 는 그림 1의 왼쪽과 같이 bounding box의 위치에서 네개의 변 까지의 거리를 뜻합니다. 

location이 여러 bounding box에 속하는 경우 ambiguous sample로 간주됩니다. regression target으로 minimal area의 bounding box를 선택하기만 하면 됩니다. 

다음은 multi-level prediction을 사용하여 ambiguous sample의 수를 크게 줄일 수 있으므로 검출 성능에 거의 영향을 미치지 않음을 보여주게 됩니다. location $(x, y)$가 bounding box $B_{i}$와 연관되면 location에 대한 training regression target은 다음과 같이 나타낼 수 있습니다. 










[FCOS: Fully Convolutional One-Stage Object Detection]: https://arxiv.org/abs/1904.01355
